{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([98304]) torch.Size([98304, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from vq_vae import DAVIS\n",
    "from vq_vae import Model as Model_VQ_VAE\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:3\")\n",
    "num_hiddens = 128\n",
    "num_residual_hiddens = 32\n",
    "num_residual_layers = 2\n",
    "embedding_dim = 64\n",
    "num_embeddings = 512\n",
    "commitment_cost = 0.25\n",
    "decay = 0.99\n",
    "learning_rate = 1e-3\n",
    "\n",
    "vq_vae = Model_VQ_VAE(num_hiddens, num_residual_layers, num_residual_hiddens,\n",
    "                      num_embeddings, embedding_dim,\n",
    "                      commitment_cost, decay).to(device)\n",
    "vq_vae.load_state_dict(torch.load('out/model_46k.pt'))\n",
    "\n",
    "\n",
    "# Dataset Parameters\n",
    "num_frames = 12  # each video uses 10 consecutive frames for training\n",
    "davis_root = '/playpen-raid2/qinliu/data/DAVIS'\n",
    "\n",
    "trainset = DAVIS(root=davis_root, num_frames=num_frames, train=True)\n",
    "valset = DAVIS(root=davis_root, num_frames=num_frames, train=False)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=1,\n",
    "                                           shuffle=True, num_workers=1)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(valset, batch_size=1,\n",
    "                                         shuffle=True, num_workers=1)\n",
    "\n",
    "vq_vae.eval()\n",
    "\n",
    "inputs, _ = next(iter(train_loader))\n",
    "inputs = torch.squeeze(inputs, dim=0)\n",
    "inputs = inputs.to(device)\n",
    "inputs_shape = inputs.shape\n",
    "\n",
    "code_indices = vq_vae.get_code_indices(inputs)\n",
    "code_indices = code_indices.view(num_frames, inputs_shape[2] // 4, inputs_shape[3] // 4)\n",
    "\n",
    "embeddings = vq_vae._vq_vae.quantize(code_indices)\n",
    "\n",
    "print(code_indices.shape, embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# train PixelCNN to generate new images\n",
    "\n",
    "class MaskedConv2d(nn.Conv2d):\n",
    "    \"\"\"\n",
    "    Implements a conv2d with mask applied on its weights.\n",
    "    \n",
    "    Args:\n",
    "        mask (torch.Tensor): the mask tensor.\n",
    "        in_channels (int): Number of channels in the input image.\n",
    "        out_channels (int):  Number of channels produced by the convolution.\n",
    "        kernel_size (int or tuple): Size of the convolving kernel\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, mask, in_channels, out_channels, kernel_size, **kwargs):\n",
    "        super().__init__(in_channels, out_channels, kernel_size, **kwargs)\n",
    "        self.register_buffer('mask', mask[None, None])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.weight.data *= self.mask # mask weights\n",
    "        return super().forward(x)\n",
    "    \n",
    "\n",
    "class VerticalStackConv(MaskedConv2d):\n",
    "\n",
    "    def __init__(self, mask_type, in_channels, out_channels, kernel_size, **kwargs):\n",
    "        # Mask out all pixels below. For efficiency, we could also reduce the kernel\n",
    "        # size in height (k//2, k), but for simplicity, we stick with masking here.\n",
    "        self.mask_type = mask_type\n",
    "        \n",
    "        if isinstance(kernel_size, int):\n",
    "            kernel_size = (kernel_size, kernel_size)\n",
    "        mask = torch.zeros(kernel_size)\n",
    "        mask[:kernel_size[0]//2, :] = 1.0\n",
    "        if self.mask_type == \"B\":\n",
    "            mask[kernel_size[0]//2, :] = 1.0\n",
    "\n",
    "        super().__init__(mask, in_channels, out_channels, kernel_size, **kwargs)\n",
    "        \n",
    "\n",
    "class HorizontalStackConv(MaskedConv2d):\n",
    "\n",
    "    def __init__(self, mask_type, in_channels, out_channels, kernel_size, **kwargs):\n",
    "        # Mask out all pixels on the left. Note that our kernel has a size of 1\n",
    "        # in height because we only look at the pixel in the same row.\n",
    "        self.mask_type = mask_type\n",
    "        \n",
    "        if isinstance(kernel_size, int):\n",
    "            kernel_size = (1, kernel_size)\n",
    "        assert kernel_size[0] == 1\n",
    "        if \"padding\" in kwargs:\n",
    "            if isinstance(kwargs[\"padding\"], int):\n",
    "                kwargs[\"padding\"] = (0, kwargs[\"padding\"])\n",
    "        \n",
    "        mask = torch.zeros(kernel_size)\n",
    "        mask[:, :kernel_size[1]//2] = 1.0\n",
    "        if self.mask_type == \"B\":\n",
    "            mask[:, kernel_size[1]//2] = 1.0\n",
    "\n",
    "        super().__init__(mask, in_channels, out_channels, kernel_size, **kwargs)\n",
    "        \n",
    "class GatedMaskedConv(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, kernel_size=3, dilation=1):\n",
    "        \"\"\"\n",
    "        Gated Convolution block implemented the computation graph shown above.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        padding = dilation * (kernel_size - 1) // 2\n",
    "        self.conv_vert = VerticalStackConv(\"B\", in_channels, 2*in_channels, kernel_size, padding=padding,\n",
    "                                          dilation=dilation)\n",
    "        self.conv_horiz = HorizontalStackConv(\"B\", in_channels, 2*in_channels, kernel_size, padding=padding,\n",
    "                                             dilation=dilation)\n",
    "        self.conv_vert_to_horiz = nn.Conv2d(2*in_channels, 2*in_channels, kernel_size=1)\n",
    "        self.conv_horiz_1x1 = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, v_stack, h_stack):\n",
    "        # Vertical stack (left)\n",
    "        v_stack_feat = self.conv_vert(v_stack)\n",
    "        v_val, v_gate = v_stack_feat.chunk(2, dim=1)\n",
    "        v_stack_out = torch.tanh(v_val) * torch.sigmoid(v_gate)\n",
    "\n",
    "        # Horizontal stack (right)\n",
    "        h_stack_feat = self.conv_horiz(h_stack)\n",
    "        h_stack_feat = h_stack_feat + self.conv_vert_to_horiz(v_stack_feat)\n",
    "        h_val, h_gate = h_stack_feat.chunk(2, dim=1)\n",
    "        h_stack_feat = torch.tanh(h_val) * torch.sigmoid(h_gate)\n",
    "        h_stack_out = self.conv_horiz_1x1(h_stack_feat)\n",
    "        h_stack_out = h_stack_out + h_stack\n",
    "\n",
    "        return v_stack_out, h_stack_out\n",
    "    \n",
    "    \n",
    "class GatedPixelCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initial first conv with mask_type A\n",
    "        self.conv_vstack = VerticalStackConv(\"A\", in_channels, channels, 3, padding=1)\n",
    "        self.conv_hstack = HorizontalStackConv(\"A\", in_channels, channels, 3, padding=1)\n",
    "        # Convolution block of PixelCNN. use dilation instead of \n",
    "        # downscaling used in the encoder-decoder architecture in PixelCNN++\n",
    "        self.conv_layers = nn.ModuleList([\n",
    "            GatedMaskedConv(channels),\n",
    "            GatedMaskedConv(channels, dilation=2),\n",
    "            GatedMaskedConv(channels)\n",
    "        ])\n",
    "        \n",
    "        # Output classification convolution (1x1)\n",
    "        self.conv_out = nn.Conv2d(channels, out_channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # first convolutions\n",
    "        v_stack = self.conv_vstack(x)\n",
    "        h_stack = self.conv_hstack(x)\n",
    "        # Gated Convolutions\n",
    "        for layer in self.conv_layers:\n",
    "            v_stack, h_stack = layer(v_stack, h_stack)\n",
    "        # 1x1 classification convolution\n",
    "        # Apply ELU before 1x1 convolution for non-linearity on residual connection\n",
    "        out = self.conv_out(F.elu(h_stack))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [0/500000]: loss 6.236263275146484\n",
      "\t [1/500000]: loss 6.204369068145752\n",
      "\t [2/500000]: loss 6.169195175170898\n",
      "\t [3/500000]: loss 6.123173236846924\n",
      "\t [4/500000]: loss 6.053569316864014\n",
      "\t [5/500000]: loss 5.939473628997803\n",
      "\t [6/500000]: loss 5.760213851928711\n",
      "\t [7/500000]: loss 5.528297424316406\n",
      "\t [8/500000]: loss 5.3283538818359375\n",
      "\t [9/500000]: loss 5.256496906280518\n",
      "\t [10/500000]: loss 5.213814735412598\n",
      "\t [11/500000]: loss 5.139461040496826\n",
      "\t [12/500000]: loss 5.0683417320251465\n",
      "\t [13/500000]: loss 5.022518157958984\n",
      "\t [14/500000]: loss 4.996445655822754\n",
      "\t [15/500000]: loss 4.978377342224121\n",
      "\t [16/500000]: loss 4.959958553314209\n",
      "\t [17/500000]: loss 4.937673091888428\n",
      "\t [18/500000]: loss 4.910678386688232\n",
      "\t [19/500000]: loss 4.879382133483887\n",
      "\t [20/500000]: loss 4.844651699066162\n",
      "\t [21/500000]: loss 4.818471431732178\n",
      "\t [22/500000]: loss 4.804126262664795\n",
      "\t [23/500000]: loss 4.763067722320557\n",
      "\t [24/500000]: loss 4.736823558807373\n",
      "\t [25/500000]: loss 4.721305847167969\n",
      "\t [26/500000]: loss 4.702123641967773\n",
      "\t [27/500000]: loss 4.675405979156494\n",
      "\t [28/500000]: loss 4.646688461303711\n",
      "\t [29/500000]: loss 4.623814105987549\n",
      "\t [30/500000]: loss 4.602686882019043\n",
      "\t [31/500000]: loss 4.576288223266602\n",
      "\t [32/500000]: loss 4.5505499839782715\n",
      "\t [33/500000]: loss 4.527308464050293\n",
      "\t [34/500000]: loss 4.505311489105225\n",
      "\t [35/500000]: loss 4.483119487762451\n",
      "\t [36/500000]: loss 4.459813117980957\n",
      "\t [37/500000]: loss 4.4358391761779785\n",
      "\t [38/500000]: loss 4.412333965301514\n",
      "\t [39/500000]: loss 4.3896660804748535\n",
      "\t [40/500000]: loss 4.367256164550781\n",
      "\t [41/500000]: loss 4.344246864318848\n",
      "\t [42/500000]: loss 4.320832252502441\n",
      "\t [43/500000]: loss 4.298061370849609\n",
      "\t [44/500000]: loss 4.275954723358154\n",
      "\t [45/500000]: loss 4.254186630249023\n",
      "\t [46/500000]: loss 4.233254909515381\n",
      "\t [47/500000]: loss 4.2132039070129395\n",
      "\t [48/500000]: loss 4.193165302276611\n",
      "\t [49/500000]: loss 4.172444820404053\n",
      "\t [50/500000]: loss 4.150583267211914\n",
      "\t [51/500000]: loss 4.128421306610107\n",
      "\t [52/500000]: loss 4.106027126312256\n",
      "\t [53/500000]: loss 4.083864688873291\n",
      "\t [54/500000]: loss 4.0623698234558105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f81f7afb1f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/playpen-raid/qinliu/tools/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/playpen-raid/qinliu/tools/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1322, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/playpen-raid/qinliu/tools/anaconda3/lib/python3.8/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/playpen-raid/qinliu/tools/anaconda3/lib/python3.8/multiprocessing/popen_fork.py\", line 44, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/playpen-raid/qinliu/tools/anaconda3/lib/python3.8/multiprocessing/connection.py\", line 931, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/playpen-raid/qinliu/tools/anaconda3/lib/python3.8/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t [55/500000]: loss 4.040699481964111\n",
      "\t [56/500000]: loss 4.018599033355713\n",
      "\t [57/500000]: loss 3.9957942962646484\n",
      "\t [58/500000]: loss 3.972363233566284\n",
      "\t [59/500000]: loss 3.9487712383270264\n",
      "\t [60/500000]: loss 3.925366163253784\n",
      "\t [61/500000]: loss 3.9021499156951904\n",
      "\t [62/500000]: loss 3.878460645675659\n",
      "\t [63/500000]: loss 3.854994058609009\n",
      "\t [64/500000]: loss 3.8317644596099854\n",
      "\t [65/500000]: loss 3.8082275390625\n",
      "\t [66/500000]: loss 3.784783363342285\n",
      "\t [67/500000]: loss 3.761204481124878\n",
      "\t [68/500000]: loss 3.737516403198242\n",
      "\t [69/500000]: loss 3.7143232822418213\n",
      "\t [70/500000]: loss 3.6914737224578857\n",
      "\t [71/500000]: loss 3.6692025661468506\n"
     ]
    }
   ],
   "source": [
    "# Train GatedPixelCNN to learn the prior of latent code indices\n",
    "num_training_updates = 500000\n",
    "\n",
    "\n",
    "pixelcnn = GatedPixelCNN(num_embeddings, 128, num_embeddings)\n",
    "pixelcnn = pixelcnn.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(pixelcnn.parameters(), lr=1e-3)\n",
    "\n",
    "# train pixelcnn\n",
    "print_freq = 500\n",
    "for i in range(num_training_updates):\n",
    "    inputs, _ = next(iter(train_loader))\n",
    "    inputs = torch.squeeze(inputs, dim=0)\n",
    "    inputs = inputs.to(device)\n",
    "    inputs_shape = inputs.shape\n",
    "\n",
    "    indices = vq_vae.get_code_indices(inputs)\n",
    "    indices = indices.view(\n",
    "        num_frames, inputs_shape[2] // 4, inputs_shape[3] // 4)\n",
    "\n",
    "    indices = indices.cuda()\n",
    "    one_hot_indices = F.one_hot(\n",
    "        indices, num_embeddings).float().permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "    outputs = pixelcnn(one_hot_indices)\n",
    "\n",
    "    loss = F.cross_entropy(outputs, indices)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # if (i + 1) % print_freq == 0 or (i + 1) == len(train_loader):\n",
    "    print(\"\\t [{}/{}]: loss {}\".format(i, num_training_updates, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e89c9d8a920cc6bbfac98998034c8a15ed2e75dae1678af787f4c4e38e5be518"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
