{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from typing import List, TypeVar, Any\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "from abc import abstractmethod\n",
    "\n",
    "Tensor = TypeVar(\"torch.tensor\")\n",
    "\n",
    "\n",
    "class BaseVAE(nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super(BaseVAE, self).__init__()\n",
    "\n",
    "    def encode(self, input: Tensor) -> List[Tensor]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def decode(self, input: Tensor) -> Any:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sample(self, batch_size:int, current_device: int, **kwargs) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, *inputs: Tensor) -> Tensor:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def loss_function(self, *inputs: Any, **kwargs) -> Tensor:\n",
    "        pass\n",
    "\n",
    "\n",
    "class VanillaVAE(BaseVAE):\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 latent_dim: int,\n",
    "                 in_spatial_H = 256,\n",
    "                 in_spatial_W = 256,\n",
    "                 num_hidden_layers = 5,\n",
    "                 hidden_dims: List = [32, 64, 128, 256, 512],\n",
    "                 **kwargs) -> None:\n",
    "        super(VanillaVAE, self).__init__()\n",
    "\n",
    "        assert len(hidden_dims) == num_hidden_layers\n",
    "\n",
    "        self.H = in_spatial_H // (2**num_hidden_layers)\n",
    "        self.W = in_spatial_W // (2**num_hidden_layers)\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.latent_dim = latent_dim\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        modules = []\n",
    "\n",
    "        # Build Encoder\n",
    "        for h_dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, out_channels=h_dim,\n",
    "                              kernel_size=3, stride=2, padding=1),\n",
    "                    nn.BatchNorm2d(h_dim),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "            in_channels = h_dim\n",
    "        modules.append(nn.MaxPool2d((self.H, self.W)))\n",
    "\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "\n",
    "        # Build Decoder\n",
    "        modules = []\n",
    "\n",
    "        self.decoder_input = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dims[-1]),\n",
    "            nn.Linear(hidden_dims[-1], hidden_dims[-1] * self.H),\n",
    "            nn.Linear(hidden_dims[-1] * self.H, hidden_dims[-1] * self.H * self.W),            \n",
    "            )\n",
    "\n",
    "        hidden_dims = self.hidden_dims[::-1]\n",
    "\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(hidden_dims[i],\n",
    "                                       hidden_dims[i + 1],\n",
    "                                       kernel_size=3,\n",
    "                                       stride=2,\n",
    "                                       padding=1,\n",
    "                                       output_padding=1),\n",
    "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "        self.final_layer = nn.Sequential(\n",
    "                            nn.ConvTranspose2d(hidden_dims[-1],\n",
    "                                               hidden_dims[-1],\n",
    "                                               kernel_size=3,\n",
    "                                               stride=2,\n",
    "                                               padding=1,\n",
    "                                               output_padding=1),\n",
    "                            nn.BatchNorm2d(hidden_dims[-1]),\n",
    "                            nn.LeakyReLU(),\n",
    "                            nn.Conv2d(hidden_dims[-1], out_channels=self.in_channels,\n",
    "                                      kernel_size= 3, padding= 1),\n",
    "                            )\n",
    "\n",
    "    def encode(self, input: Tensor) -> List[Tensor]:\n",
    "        \"\"\"\n",
    "        Encodes the input by passing through the encoder network\n",
    "        and returns the latent codes.\n",
    "        :param input: (Tensor) Input tensor to encoder [N x C x H x W]\n",
    "        :return: (Tensor) List of latent codes\n",
    "        \"\"\"\n",
    "        result = self.encoder(input)\n",
    "        result = torch.flatten(result, start_dim=1)\n",
    "\n",
    "        # Split the result into mu and var components\n",
    "        # of the latent Gaussian distribution\n",
    "        mu = self.fc_mu(result)\n",
    "        log_var = self.fc_var(result)\n",
    "\n",
    "        return [mu, log_var]\n",
    "\n",
    "    def decode(self, z: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Maps the given latent codes\n",
    "        onto the image space.\n",
    "        :param z: (Tensor) [B x D]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "        result = self.decoder_input(z)\n",
    "        result = result.view(z.shape[0], self.hidden_dims[-1], self.H, self.W)\n",
    "        result = self.decoder(result)\n",
    "        result = self.final_layer(result)\n",
    "        return result\n",
    "\n",
    "    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Reparameterization trick to sample from N(mu, var) from\n",
    "        N(0,1).\n",
    "        :param mu: (Tensor) Mean of the latent Gaussian [B x D]\n",
    "        :param logvar: (Tensor) Standard deviation of the latent Gaussian [B x D]\n",
    "        :return: (Tensor) [B x D]\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, input: Tensor, **kwargs) -> List[Tensor]:\n",
    "        mu, log_var = self.encode(input)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return  [self.decode(z), input, mu, log_var]\n",
    "\n",
    "    def loss_function(self,\n",
    "                      *args,\n",
    "                      **kwargs) -> dict:\n",
    "        \"\"\"\n",
    "        Computes the VAE loss function.\n",
    "        KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        recons = args[0]\n",
    "        input = args[1]\n",
    "        mu = args[2]\n",
    "        log_var = args[3]\n",
    "\n",
    "        kld_weight = kwargs['M_N'] # Account for the minibatch samples from the dataset\n",
    "        recons_loss =F.mse_loss(recons, input)\n",
    "\n",
    "\n",
    "        kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
    "\n",
    "        loss = recons_loss + kld_weight * kld_loss\n",
    "        return {'loss': loss, 'Reconstruction_Loss':recons_loss.detach(), 'KLD':-kld_loss.detach()}\n",
    "\n",
    "    def sample(self,\n",
    "               num_samples:int,\n",
    "               current_device: int, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Samples from the latent space and return the corresponding\n",
    "        image space map.\n",
    "        :param num_samples: (Int) Number of samples\n",
    "        :param current_device: (Int) Device to run the model\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "        z = torch.randn(num_samples,\n",
    "                        self.latent_dim)\n",
    "\n",
    "        z = z.to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Given an input image x, returns the reconstructed image\n",
    "        :param x: (Tensor) [B x C x H x W]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-jmlu_a6p because the default path (/home/qinliu19/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import torchvision.datasets.vision as vision\n",
    "import torchvision.io as io\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
    "\n",
    "\n",
    "class DAVIS(vision.VisionDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        num_frames: int = 10,\n",
    "        train: bool = True,\n",
    "        transform: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None,\n",
    "    ) -> None:\n",
    "        super().__init__(root, transform=transform, target_transform=target_transform)\n",
    "        self.num_frames = num_frames\n",
    "        self.train = train  # training set or test set\n",
    "        self.root = root\n",
    "\n",
    "        self.video_names = []\n",
    "        if train:\n",
    "            with open(os.path.join(root, 'ImageSets/2017/train.txt'), 'r') as f:\n",
    "                for line in f:\n",
    "                    self.video_names.append(line.strip('\\n'))\n",
    "\n",
    "        else:\n",
    "            with open(os.path.join(root, 'ImageSets/2017/val.txt'), 'r') as f:\n",
    "                for line in f:\n",
    "                    self.video_names.append(line.strip('\\n'))\n",
    "\n",
    "    def __getitem__(self, index: int) -> Any:\n",
    "        video_name = self.video_names[index]\n",
    "        video_frames = list((Path(self.root)/'JPEGImages/480p/{}'.format(video_name)).glob('*.jpg'))\n",
    "        video_frames.sort()\n",
    "\n",
    "        start_frame = random.randrange(len(video_frames) - self.num_frames)\n",
    "        video_frames = video_frames[start_frame: start_frame + self.num_frames]    \n",
    "\n",
    "        video_frames_data = self._load_data(video_frames)\n",
    "        video_frames_target = 0 # we don't need target\n",
    "\n",
    "        # random crop\n",
    "        random_crop = transforms.RandomCrop((480, 840), pad_if_needed=True)\n",
    "        resize = transforms.Resize((256, 512))\n",
    "        video_frames_data = resize(random_crop(video_frames_data))\n",
    "        video_frames_data = video_frames_data / 255 # maximum value for all images in DAVIS\n",
    "\n",
    "        return video_frames_data, video_frames_target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_names)\n",
    "\n",
    "    def _load_data(self, video_frames):\n",
    "        video_frames_data = []\n",
    "        for video_frame in video_frames:\n",
    "            frame_data = io.read_image(str(video_frame), mode=io.ImageReadMode.RGB)\n",
    "            video_frames_data.append(frame_data)\n",
    "        return torch.stack(video_frames_data, dim=0).float()\n",
    "\n",
    "\n",
    "# # build mnist datasets\n",
    "# mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, \n",
    "#                                 transform=transforms.ToTensor())\n",
    "# mnist_testset = datasets.MNIST(root='./data', train=False, download=True, \n",
    "#                                transform=transforms.ToTensor())\n",
    "\n",
    "# build DAVIS dataset\n",
    "davis_root = '/playpen-raid2/qinliu/data/DAVIS'\n",
    "trainset = DAVIS(root=davis_root, num_frames=10, train=True)\n",
    "valset = DAVIS(root=davis_root, num_frames=1, train=False)\n",
    "\n",
    "mb_size = 1\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=mb_size, \n",
    "                                           shuffle=True, num_workers=1)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(valset, batch_size=mb_size, \n",
    "                                          shuffle=False, num_workers=1)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-0; Loss: 2.564e+05\n",
      "Iter-10; Loss: 1.641e+05\n",
      "Iter-20; Loss: 1.6e+05\n",
      "Iter-30; Loss: 2.004e+05\n",
      "Iter-40; Loss: 1.691e+05\n",
      "Iter-50; Loss: 1.766e+05\n",
      "Iter-60; Loss: 8.523e+04\n",
      "Iter-70; Loss: 1.09e+05\n",
      "Iter-80; Loss: 1.541e+05\n",
      "Iter-90; Loss: 1.415e+05\n",
      "Iter-100; Loss: 1.239e+05\n",
      "Iter-110; Loss: 1.618e+05\n",
      "Iter-120; Loss: 1.23e+05\n",
      "Iter-130; Loss: 7.637e+04\n",
      "Iter-140; Loss: 9.642e+04\n",
      "Iter-150; Loss: 7.238e+04\n",
      "Iter-160; Loss: 1.446e+05\n",
      "Iter-170; Loss: 7.728e+04\n",
      "Iter-180; Loss: 9.998e+04\n",
      "Iter-190; Loss: 7.787e+04\n",
      "Iter-200; Loss: 7.784e+04\n",
      "Iter-210; Loss: 9.859e+04\n",
      "Iter-220; Loss: 7.433e+04\n",
      "Iter-230; Loss: 1.106e+05\n",
      "Iter-240; Loss: 8.743e+04\n",
      "Iter-250; Loss: 1.058e+05\n",
      "Iter-260; Loss: 1.271e+05\n",
      "Iter-270; Loss: 5.541e+04\n",
      "Iter-280; Loss: 1.192e+05\n",
      "Iter-290; Loss: 1.02e+05\n",
      "Iter-300; Loss: 7.195e+04\n",
      "Iter-310; Loss: 1.08e+05\n",
      "Iter-320; Loss: 9.399e+04\n",
      "Iter-330; Loss: 5.671e+04\n",
      "Iter-340; Loss: 7.315e+04\n",
      "Iter-350; Loss: 6.532e+04\n",
      "Iter-360; Loss: 7.943e+04\n",
      "Iter-370; Loss: 6.7e+04\n",
      "Iter-380; Loss: 9.03e+04\n",
      "Iter-390; Loss: 1.079e+05\n",
      "Iter-400; Loss: 1.116e+05\n",
      "Iter-410; Loss: 8.343e+04\n",
      "Iter-420; Loss: 5.804e+04\n",
      "Iter-430; Loss: 7.318e+04\n",
      "Iter-440; Loss: 7.808e+04\n",
      "Iter-450; Loss: 7.134e+04\n",
      "Iter-460; Loss: 1.128e+05\n",
      "Iter-470; Loss: 6.155e+04\n",
      "Iter-480; Loss: 5.759e+04\n",
      "Iter-490; Loss: 1.16e+05\n",
      "Iter-500; Loss: 6.368e+04\n",
      "Iter-510; Loss: 9.386e+04\n",
      "Iter-520; Loss: 4.932e+04\n",
      "Iter-530; Loss: 7.551e+04\n",
      "Iter-540; Loss: 6.718e+04\n",
      "Iter-550; Loss: 8.809e+04\n",
      "Iter-560; Loss: 9.347e+04\n",
      "Iter-570; Loss: 7.832e+04\n",
      "Iter-580; Loss: 6.742e+04\n",
      "Iter-590; Loss: 8.427e+04\n",
      "Iter-600; Loss: 9.373e+04\n",
      "Iter-610; Loss: 1.024e+05\n",
      "Iter-620; Loss: 7.906e+04\n",
      "Iter-630; Loss: 8.157e+04\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3f8e2cac868f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreparameterize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mX_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;31m# X_sample = torch.sigmoid(X_sample)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-673de2bb4d24>\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/playpen-raid/qinliu/tools/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/playpen-raid/qinliu/tools/anaconda3/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/playpen-raid/qinliu/tools/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/playpen-raid/qinliu/tools/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    923\u001b[0m             input, output_size, self.stride, self.padding, self.kernel_size, self.dilation)  # type: ignore[arg-type]\n\u001b[1;32m    924\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m         return F.conv_transpose2d(\n\u001b[0m\u001b[1;32m    926\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             output_padding, self.groups, self.dilation)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_frames = 10\n",
    "model = VanillaVAE(3, 128, num_frames=num_frames, in_spatial_H=256, in_spatial_W=512, \\\n",
    "    num_hidden_layers=6, hidden_dims=[8, 16, 32, 64, 128, 256])\n",
    "\n",
    "c = 0\n",
    "lr = 1e-4\n",
    "\n",
    "solver = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for it in range(10000):\n",
    "    X, _ = next(iter(train_loader))\n",
    "    X = torch.squeeze(X, dim=0)\n",
    "\n",
    "    # Forward\n",
    "    z_mu, z_var = model.encode(X)    \n",
    "\n",
    "    z = model.reparameterize(z_mu, z_var)\n",
    "    X_sample = model.decode(z)\n",
    "    # X_sample = torch.sigmoid(X_sample)\n",
    "\n",
    "    # Loss\n",
    "    recon_loss = F.l1_loss(X_sample, X, reduction='sum') / num_frames\n",
    "    # recon_loss = F.binary_cross_entropy(X_sample, X, reduction='sum') / mb_size\n",
    "    kl_loss = torch.mean(0.5 * torch.sum(torch.exp(z_var) + z_mu**2 - 1. - z_var, 1))\n",
    "    loss = recon_loss + kl_loss\n",
    "\n",
    "    # Backward\n",
    "    loss.backward()\n",
    "\n",
    "    # Update\n",
    "    solver.step()\n",
    "\n",
    "    # Housekeeping\n",
    "    solver.zero_grad()\n",
    "\n",
    "    # Print and plot every now and then\n",
    "    if it % 10 == 0:\n",
    "        print('Iter-{}; Loss: {:.4}'.format(it, loss.item()))\n",
    "\n",
    "        samples = model.decode(z)\n",
    "        samples = torch.sigmoid(samples)\n",
    "        samples = samples.data.numpy()[:9]\n",
    "        # targets = X.data.numpy()[:9]\n",
    "\n",
    "        fig = plt.figure(figsize=(12, 12))\n",
    "        gs = gridspec.GridSpec(3, 3)\n",
    "        gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "        for i, sample in enumerate(samples):\n",
    "            ax = plt.subplot(gs[i])\n",
    "            plt.axis('off')\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "            ax.set_aspect('equal')\n",
    "            sample = np.moveaxis(sample, 0, -1)\n",
    "            # target = np.moveaxis(targets[i], 0, -1)\n",
    "            plt.imshow(sample)\n",
    "            # plt.imshow(target)\n",
    "\n",
    "        if not os.path.exists('out/'):\n",
    "            os.makedirs('out/')\n",
    "\n",
    "        plt.savefig('out/{}.png'.format(str(c).zfill(3)), bbox_inches='tight')\n",
    "        c += 1\n",
    "        plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e89c9d8a920cc6bbfac98998034c8a15ed2e75dae1678af787f4c4e38e5be518"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
